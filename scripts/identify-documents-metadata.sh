#!/bin/bash
###########################################################################################################
# Script to identify document files on a website, by crawling it, 
# and extract metadata from identified files
#
# Dependencies:
#	https://github.com/hakluke/hakrawler
#   https://exiftool.org/
###########################################################################################################
# Context tunning
DOCUMENT_FILES_FITLERING_REGEX="\.(pdf|docx|doc|xls|xlsx|ppt|pptx)$"
METADATA_FITLERING_REGEX="(Author|Title|Creator|Producer).*"
####
# Constants
TIMEOUT=20
DEPTH=20
UA="Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0"
WORK_FILE="/tmp/work.txt"
WORK_DIR="/tmp/work"
# Utilities functions
function write_step(){
    echo -e "\e[93m[+] $1\e[0m"
}

# Entry point
if [ "$#" -lt 1 ]; then
    script_name=$(basename "$0")
    echo "Usage:"
    echo "   $script_name [SITE_URL]"
    echo ""
    echo "Call example:"
    echo "    $script_name https://myapp.com"
    exit 1
fi
URL=$1
write_step "Searching for documents '$DOCUMENT_FILES_FITLERING_REGEX' on site '$URL'..."
cdir=$(pwd)
rm -f $WORK_FILE 2>/dev/null
echo $URL | hakrawler -d $DEPTH -insecure -u -timeout $TIMEOUT -i -h "User-Agent: $UA;" | grep -iE "$DOCUMENT_FILES_FITLERING_REGEX" > $WORK_FILE
count=$(wc -l $WORK_FILE | cut -d' ' -f1)
if [ $count -ne 0 ];
then
	echo "$count found."
	write_step "Download the files and search for metadata..."
	rm -rf $WORK_DIR 2>/dev/null
	mkdir $WORK_DIR
	cd $WORK_DIR
	wget -q -i $WORK_FILE
	exiftool * | grep -oiP "$METADATA_FITLERING_REGEX" | sort -u
	cd $cdir
else
	echo "No document found."
fi
write_step "Cleanup..."
rm -rf $WORK_DIR 2>/dev/null
rm -f $WORK_FILE 2>/dev/null
